@Phdthesis{villarreal_construction_2016,
  ADDRESS = {Davis, CA},
  AUTHOR = {Villarreal, Dan},
  SCHOOL = {University of California, Davis},
  TITLE = {The construction of social meaning: {A} matched-guise investigation of the {California} {Vowel} {Shift}},
  TYPE = {{PhD} dissertation},
  YEAR = {2016},
  ABSTRACT = {Research on social meaning, which links language variation to the wider social world, often bases claims about the social meanings of linguistic forms on production (i.e., speakers’ situational use of meaningful forms). In the case of the California Vowel Shift (CVS), an ongoing restructuring of the vowel system of California English that takes place below the level of conscious awareness, previous production research has suggested that the CVS carries social meanings of carefreeness, femininity, and privilege. Left unclear in these production-based claims is whether listeners actually pick up on and recognize the social meanings that speakers apparently utilize the CVS to transmit. In this research, a dialect recognition task with matched guises (California-shifted vs. conservative) forms the basis for exploring Californian listeners’ reactions to the CVS, and how these reactions are mediated by perceptions of dialect geography. In short, this research focuses on listeners’ reactions to the CVS in order to address a more fundamental question: How do listeners and speakers together participate in the construction of social meaning?

Stimuli for the main study task were drawn from excerpts of sociolinguistic interviews with 12 lifelong California English speakers from three regions of the state: the San Francisco Bay Area, Lower Central Valley, and Southern California. Guises were created from interview excerpts by modifying the F2 of each TRAP and GOOSE token via source-filter resynthesis methods. Californian guises featured backed TRAP and fronted GOOSE; conservative guises featured fronted TRAP and backed GOOSE. Ninety-seven Californians participated in a perceptual task in which they attempted to identify speakers’ regional origin and rated speakers on affective scales. The results indicated that Californians recognize the CVS as Californian, as California-shifted guises were less likely to be identified as from outside California (but more likely to be identified as from Southern California). Listeners rated California-shifted guises higher on the scales \textit{Californian}, \textit{sounds like a Valley girl}, and \textit{confident}, indicating a core of social meanings indexed by the CVS. Among listeners from the San Francisco Bay Area, the CVS indexes masculinity, but among Southern California listeners, the CVS indexes femininity. Listeners from across California also rated speakers who they believed to be from the same region as them higher on \textit{Californian}, \textit{familiar}, and \textit{sounds like me}.

This research demonstrates that the social meanings of linguistic forms do not reside only in speakers’ situational use of these forms, as listeners did not associate the CVS with carefreeness, femininity, or privilege, the social meanings of the CVS suggested by previous studies of California English production; instead, I propose an account of the indexical field that links perception and production by placing the core social meanings of the CVS uncovered by this research (Californian identity, sounding like a Valley girl, and confidence) at the center of the CVS’s indexical field. This research also contributes to theory in perceptual dialectology and language change. In order to explain this study’s finding that the CVS is associated with Southern California, this research introduces the perceptual-dialectological process of centrality: the identification of speakers who are believed to most exemplify the speech of a given region. Finally, this research suggests an attitudinal stance that allows changes from below such as the CVS to flourish: speakers are aware of the change in the community (at a tacit level, if not consciously) but do not believe that they are participating in the change.},
  FILE = {/pubs/Villarreal - 2016 - The construction of social meaning A matched-guis.pdf},
  DOI = {10.13140/RG.2.2.12584.32001},
  HEADING = {PhD dissertation},
  REPO = {@villarreal\_vowel\_2016}
}


@Article{villarreal_construction_2018,
  AUTHOR = {Villarreal, Dan},
  JOURNAL = {Journal of English Linguistics},
  NUMBER = {1},
  PAGES = {52--78},
  TITLE = {The construction of social meaning: {A} matched-guise investigation of the {California} {Vowel} {Shift}},
  VOLUME = {46},
  YEAR = {2018},
  ABSTRACT = {Past production research on the California Vowel Shift (CVS) has suggested that the CVS carries social meanings of carefreeness, Whiteness, femininity, and privilege (e.g., Eckert 2008b), but it is unclear whether these social meanings reflect listener perceptions. In the present study, Californian listeners heard speech samples, guessed where speakers were from, and rated speakers on language attitudes scales; stimuli in this task were matched guises differing by speakers’ use of two CVS features. The results indicated that listeners associate these features with Californianness, sounding like a Valley girl, and (for male speakers) confidence, complicating the social meanings suggested by production studies. I discuss these results in terms of how interaction context guides the perception of social meaning by activating subsets of the indexical field. This research also introduces two innovative methods for investigating sociolinguistic perception: stimuli created using resynthesized vowels within spontaneous speech produced by multiple speakers, and statistical inference via Bayesian hierarchical modeling.},
  FILE = {/pubs/Villarreal - 2018 - The construction of social meaning A matched-guis.pdf},
  DOI = {10.1177/0075424217753520},
  KEYWORDS = {sociolinguistics,American dialects,language attitudes,perceptual dialectology,phonetics},
  HEADING = {Peer-reviewed publications}
}


@Inproceedings{villarreal_modelling_2019,
  ADDRESS = {Melbourne},
  AUTHOR = {Villarreal, Dan and Clark, Lynn and Hay, Jennifer},
  EDITOR = {Calhoun, Sasha and Escudero, Paola and Tabain, Marija and Warren, Paul},
  PAGES = {1893--1897},
  TITLE = {Modelling gradience in {English} /r/ via statistical classification},
  YEAR = {2019},
  ABSTRACT = {This experiment assessed the validity of a statistical classification method for automated coding of sociophonetic variables, in particular the presence vs. absence of English non-prevocalic /r/. A random forest classifier was trained on 180 acoustic measures from 5,355 tokens of /r/ (hand-coded as Present or Absent) in a variety of English with variable rhoticity; the classifier achieved 87.9\% accuracy on training data. The classifier was then used to predict the probability of /r/ presence in 32,099 additional tokens from the same variety.

Eleven phonetically trained listeners judged 60 classifier-coded tokens as Present or Absent. Judgment results indicated a significant positive linear relationship between classifier probability and human judgments; classifier probability also outperformed individual acoustic measures (e.g., F3 minimum) in predicting human judgments. These results both validate this random forest classifier method for automated coding of sociophonetic variables and indicate the viability of modelling phonetic variation using classifier probability},
  FILE = {/pubs/Villarreal et al. - 2019 - Modelling gradience in English r via statistical.pdf},
  HEADING = {Publications in conference proceedings}
}


@Misc{villarreal_how_2019,
  AUTHOR = {Villarreal, Dan and Clark, Lynn and Hay, Jennifer and Watson, Kevin},
  TITLE = {How to train your classifier},
  YEAR = {2019},
  ABSTRACT = {(First paragraph) This guide walks readers through the process of training a random-forest classifier for automated coding of sociophonetic variation, using the statistical computing language R (R Core Team, 2018) and the packages ranger (Wright \& Ziegler, 2017) and caret (Kuhn, 2018). This guide is intended to be a companion to our 2020 Laboratory Phonology article “From categories to gradience: Auto-coding sociophonetic variation with random forests”, and it recreates the classifier of Southland English non-prevocalic /r/ discussed in the article. By consulting the R Markdown file that generated this document, readers can run the code on their own systems to recreate the /r/ classifier.},
  URL = {https://nzilbb.github.io/How-to-Train-Your-Classifier/How_to_Train_Your_Classifier.html},
  HEADING = {Open research tools}
}


@Article{holliday_intonational_2020,
  AUTHOR = {Holliday, Nicole and Villarreal, Dan},
  JOURNAL = {Laboratory Phonology},
  NUMBER = {1},
  PAGES = {1--21},
  TITLE = {Intonational variation and incrementality in listener judgments of ethnicity},
  VOLUME = {11},
  YEAR = {2020},
  ABSTRACT = {The current study examines how listeners make gradient and variable ethnolinguistic judgments in an experimental context where the speaker’s identity is well-known. It features an openguise experiment (Soukup, 2013) that assessed whether sociolinguistic judgments are subject to incrementality, with judgments increasing in magnitude as variable stimuli demonstrate more extreme differences. In particular, this task tested whether judgments of President Barack Obama as sounding ‘more’ or ‘less’ black (e.g., Alim \& Smitherman, 2012) are sensitive to differences in intonation. Half of critical stimuli featured an L+H* pitch accent, which occurs more frequently in African American Language than in Mainstream U.S. English (Holliday, 2016). Four stimuli apiece were created from these phrases by making each pitch accent more extreme by semitonebased F0 steps. Seventy-nine listeners rated these stimuli via the question, “How black does Obama sound here?” Mixed-effects modeling indicated that listeners rated more phonetically extreme L+H* stimuli as sounding blacker, regardless of listener identity. A post-hoc analysis found that listeners attended to different voice quality features in L+H* stimuli. We discuss implications for research in intonation, ethnic identification, incrementality, language attitudes, and sociolinguistic awareness.},
  FILE = {/pubs/Holliday and Villarreal - 2020 - Intonational variation and incrementality in liste.pdf},
  DOI = {10.5334/labphon.229},
  COPYRIGHT = {Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC-BY-NC-SA)},
  HEADING = {Peer-reviewed publications}
}


@Article{villarreal_categories_2020,
  AUTHOR = {Villarreal, Dan and Clark, Lynn and Hay, Jennifer and Watson, Kevin},
  JOURNAL = {Laboratory Phonology},
  NUMBER = {6},
  PAGES = {1--31},
  TITLE = {From categories to gradience: {Auto}-coding sociophonetic variation with random forests},
  VOLUME = {11},
  YEAR = {2020},
  ABSTRACT = {The time-consuming nature of coding sociophonetic variables that are typically treated as categorical represents an impediment to addressing research questions around these variables that require large volumes of data. In this paper, we apply a machine learning method, random forest classification (Breiman, 2001), to automate coding (categorical prediction) of two English sociophonetic variables traditionally treated as categorical, non-prevocalic /r/ and word-medial intervocalic /t/, based on tokens’ acoustic signatures. We found good performance for binary classifiers of non-prevocalic /r/ (Absent versus Present) and medial /t/ (Voiced versus Voiceless), but not for medial /t/ with a six-way coding distinction (largely due to some codes being sparsely represented in the training data). This method also yields rankings of acoustic measures in terms of importance in classification. Beyond any individual measures, this method generates probabilistic predictions of variation (classifier probabilities) that represent a composite of the acoustic cues fed into the model. In a listening experiment, we found that not only did classifier probabilities significantly capture gradience in trained listeners’ perceptions of rhoticity, they better predicted listeners’ perceptions than individual acoustic measures. This method thus represents a new approach to reconciling the categorical and continuous dimensions of sociophonetic variation.},
  FILE = {/pubs/Villarreal et al. - 2020 - From categories to gradience Auto-coding sociopho.pdf},
  DOI = {10.5334/labphon.216},
  HEADING = {Peer-reviewed publications}
}


@Article{villarreal_local_2021,
  AUTHOR = {Villarreal, Dan and Kohn, Mary},
  JOURNAL = {American Speech},
  NUMBER = {1},
  PAGES = {45--77},
  TITLE = {Local meanings for supra-local change: {Perceptions} of {TRAP} backing in {Kansas}},
  VOLUME = {96},
  YEAR = {2021},
  ABSTRACT = {While the retraction of trap is found throughout the American West, it is primarily associated with California and supposed Californian values in both the popular media and the ears of Californian listeners. This study investigates the local construction of meaning for a supralocal sound change by examining perceptions of trap backing in Kansas, a locale that has also undergone front lax vowel retraction. Thirty-five college students heard matched-guise stimuli differing only by trap F2, guessed speakers’ regional origin, and rated speakers on 14 affective scales. Listeners associated trap backing with California (despite local participation in the sound shift) and general prestige. The authors suggest that this association with general prestige may help to explain the presence of this vowel shift in Kansas despite considerable ideological differences with California. They argue that these results highlight the interaction between local construction of meaning and broader national discourses for a sound change: while stereotypical associations with a sound change can spread rapidly through means like popular media, stance and identity associations are constructed at the local level.},
  FILE = {/pubs/Villarreal and Kohn - 2021 - Local meanings for supra-local change Perceptions.pdf},
  DOI = {10.1215/00031283-8186897},
  HEADING = {Peer-reviewed publications}
}


@Article{villarreal_sociolinguistic_forthcoming,
  AUTHOR = {Villarreal, Dan},
  JOURNAL = {Linguistics Vanguard},
  TITLE = {Sociolinguistic auto-coding has fairness problems too: {Measuring} and mitigating bias.},
  YEAR = {forthcoming},
  ABSTRACT = {Sociolinguistics researchers can use sociolinguistic auto-coding (SLAC) to predict humans’ hand-codes of sociolinguistic data. While auto-coding promises opportunities for greater efficiency, like other computational methods there are inherent concerns about this method’s fairness—whether it generates equally valid predictions for different speaker groups. This would be problematic for sociolinguistic work given the central importance of correlating speaker groups to differences in variable usage. The current study examines SLAC fairness through the lens of gender fairness in auto-coding Southland New Zealand English non-prevocalic /r/. First, given that there are multiple, mutually incompatible definitions of machine learning fairness, I argue that fairness for SLAC is best captured by two fairness definitions (overall accuracy equality and class accuracy equality) corresponding to three fairness metrics. Second, I empirically assess the extent to which SLAC is prone to unfairness. I find that a specific auto-coder described in previous literature performed poorly on all three fairness metrics. Third, to remedy these imbalances, I tested unfairness mitigation strategies on the same data. I find several strategies that reduced unfairness to virtually zero. I close by discussing what SLAC fairness means not just for auto-coding, but more broadly for how we conceptualize variation as an object of study.},
  FILE = {/pubs/Villarreal - Sociolinguistic auto-coding has fairness problems .pdf},
  COPYRIGHT = {All rights reserved},
  HEADING = {Works in progress},
  REPO = {@villarreal\_slac-fairness\_2023}
}


@Misc{villarreal_slac-fairness_2023,
  AUTHOR = {Villarreal, Dan},
  TITLE = {{SLAC}-{Fairness}: {Tools} to assess fairness and mitigate unfairness in sociolinguistic auto-coding},
  YEAR = {2023},
  ABSTRACT = {This GitHub repository is a companion to the paper "Sociolinguistic auto-coding has fairness problems too: Measuring and mitigating overlearning bias", forthcoming in \textit{Linguistics Vanguard}. In the paper, I investigate \textbf{sociolinguistic auto-coding (SLAC)} through the lens of \textbf{machine-learning fairness}. Just as some algorithms produce biased predictions by overlearning group characteristics, I find that the same is true for SLAC. As a result, I attempt \textbf{unfairness mitigation strategies (UMSs)} as techniques for removing gender bias in auto-coding predictions (without harming overall auto-coding performance too badly).

What's the point of this repository?
First, you can \textbf{reproduce} the analysis I performed for the \textit{Linguistics Vanguard} paper, using the same data and code that I did. Simply follow the analysis walkthrough tutorial.
Second, you can also adapt this code to your own projects. You might want to use it if you want to (1) \textbf{assess fairness} for a pre-existing auto-coder and/or (2) create a \textbf{fair auto-coder} by testing unfairness mitigation strategies on your data.
Finally, I invite comments, critiques, and questions about this code. I've made this code available for transparency's sake, so please don't hesitate to reach out!},
  URL = {https://github.com/djvill/SLAC-Fairness, https://djvill.github.io/SLAC-Fairness/},
  HEADING = {Open research tools}
}


@Misc{villarreal_vowel_2016,
  AUTHOR = {Villarreal, Dan},
  TITLE = {Vowel manipulation toolkit},
  YEAR = {2016},
  ABSTRACT = {This folder contains files useful for replicating the vowel manipulation methods used in my 2016 dissertation.},
  URL = {https://github.com/djvill/Vowel-Manipulation},
  HEADING = {Open research tools}
}


@Inproceedings{villarreal_modeling_2023,
  ADDRESS = {Prague},
  AUTHOR = {Villarreal, Dan and Grama, James},
  TITLE = {Modeling social meanings of phonetic variation amid variable co-occurrence: {A} machine-learning approach},
  YEAR = {2023},
  ABSTRACT = {The social meaning of phonetic variation is central to sociophonetics. Despite the co-occurrence of multiple potentially meaningful variables in speech, previous experimental approaches generally focus on single features. This analysis investigated whether machine-learning methods can uncover vowel features that impact sociolinguistic perceptions. Ninety-seven listeners rated short, spontaneous California English stimuli on 12 affective scales; /æ/ and /u/ tokens were manipulated to create “California-shifted” and “unshifted” guises, with other vowels left to vary naturally. Mirroring “bag-of-words” approaches to text corpora, we treated stimuli as “bags of features” based on 11 vowel phonemes. We used the Boruta feature-selection algorithm to assess the importance of these features, plus guise, on affective scale ratings. The most frequently selected variables included both predictable (/æ/) and less well-attested variables (/ʊ/). However, guise was never selected, suggesting it was an overly-coarse axis of relevant variation. We argue that “bottom-up” approaches can model social meanings amid variable co-occurrence.},
  FILE = {/pubs/Villarreal and Grama - 2023 - Modeling social meanings of phonetic variation ami.pdf},
  HEADING = {Publications in conference proceedings}
}

